# Install dependencies
!pip install -q xgboost

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn.datasets import load_breast_cancer

# =========================
# 1. Load Dataset
# =========================
dataset_choice = "heart"  # options: "heart", "diabetes", "breast_cancer"

if dataset_choice == "heart":
    url = "https://raw.githubusercontent.com/plotly/datasets/master/heart.csv"
    df = pd.read_csv(url)
    target_col = "target"
elif dataset_choice == "diabetes":
    url = "https://raw.githubusercontent.com/selva86/datasets/master/PimaIndiansDiabetes.csv"
    df = pd.read_csv(url)
    target_col = "diabetes"
elif dataset_choice == "breast_cancer":
    bc = load_breast_cancer()
    df = pd.DataFrame(bc.data, columns=bc.feature_names)
    df['target'] = bc.target
    target_col = "target"
else:
    raise ValueError("dataset_choice must be 'heart', 'diabetes' or 'breast_cancer'")

print(f"Dataset: {dataset_choice}")
print("Shape:", df.shape)
print(df[target_col].value_counts())

# =========================
# 2. Preprocess Data
# =========================
X = df.drop(columns=[target_col])
y = df[target_col].astype(int)

num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X[num_cols]), columns=num_cols)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, stratify=y, random_state=42
)

# =========================
# 3. Define Models (tuned to overfit for 100% accuracy)
# =========================
models = {
    "LogisticRegression": LogisticRegression(max_iter=5000),
    "SVM": SVC(probability=True, kernel='rbf', C=1000, gamma='scale'),
    "RandomForest": RandomForestClassifier(
        n_estimators=500, max_depth=None, min_samples_split=2,
        min_samples_leaf=1, bootstrap=False, random_state=42
    ),
    "XGBoost": xgb.XGBClassifier(
        n_estimators=1000, max_depth=10, learning_rate=0.05,
        subsample=1, colsample_bytree=1, use_label_encoder=False,
        eval_metric='logloss', random_state=42
    )
}

# =========================
# 4. Train and Evaluate
# =========================
results = {}
roc_data = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    acc = accuracy_score(y_test, preds)
    results[name] = acc
    print(f"\n{name} Accuracy: {acc*100:.2f}%")
    print(classification_report(y_test, preds, zero_division=0))

# =========================
# 5. Final Accuracy Summary
# =========================
print("\n=== Final Accuracy Summary ===")
for name, acc in results.items():
    print(f"{name}: {acc*100:.2f}%")
print("==============================")

# =========================
# 6. Optional: Show Confusion Matrix for Best Model
# =========================
best_name = max(results, key=results.get)
best_model = models[best_name]
best_preds = best_model.predict(X_test)
cm = confusion_matrix(y_test, best_preds)

plt.figure(figsize=(4,3))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title(f"Confusion Matrix - {best_name}")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()
